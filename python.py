# -*- coding: utf-8 -*-
"""notebook95f89e4014.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17oYp3xGn3VRR94kyW4Akg2-AjDWt2-J8

### Importing the libraries
"""

import torch 
import numpy as np
import pandas as pd

!pip install transformers

"""### Getting the training and validation data"""

# Train data
data =  pd.read_csv("../input/shl123/train_data.csv")
data.sample(10)

## Total No. of training sentences
len(data)

# List of sentences and their labels
sentences = data.input.values
labels = data.labels.values

## Check how many negative samples and positive samples there are
incorrect  = data[data.labels==0]
correct = data[data.labels==1]

print("0 labelled sentences are: ",len(incorrect))
print("1 labelled sentences are: ",len(correct))

"""So we have equal no. of positive and negative samples which ensures no bias of one particular class in the dataset"""

## Helper function to find max length sentence and average length
def len_calcs(data):
  max_len = -1 
  max_sent = "" # Max length sentence
  lens = 0 # Total words in entire corpus
  for sentence in sentences:
    if len(sentence)>max_len:
      max_len = len(sentence.split(" "))
      max_sent = sentence
    lens+=len(sentence.split(" "))
  print(f"The max length sentence is {max_sent} of length {max_len}")
  print(f"The average length is {lens/len(data)}")
len_calcs(data)

"""### BERT TOKENIZER

To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.
Reference: https://arxiv.org/abs/1810.04805
"""

# Loading the BERT Tokenizer
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_cased=True)

# Original sentence
print('Original: ',sentences[1])

# Sentence split into tokens

print("Tokenized: ",tokenizer.tokenize(sentences[1]))

## We now tokenize the entire training dataset and map the tokens to their ID's

def create_dataset(sentences,labels):
  ids = []
  atten_masks = []

  for sentence in sentences:
    ''' encode_plus: (1) Tokenize the sentence
                    (2) Prepend [CLS] start token to the beginning to indicate start of sentence
                    (3) Append [SEP] end token to the end
                    (4) Map tokens to IDs
                    (5) Pad or truncate sentence to max_length
                    (6) Create attention masks to ignore [PAD] tokens
    '''
    encoded_sentences = tokenizer.encode_plus(sentence,
                                              add_special_tokens=True, #Add [CLS and [SEP]
                                              max_length=128,
                                              pad_to_max_length=True,
                                              return_attention_mask = True,
                                              return_tensors = 'pt' # Return pytorch tensors
                                              )
    # Add the encoded sentence to the list. 
    ids.append(encoded_sentences['input_ids'])
    # Attention mask differentiates padding from non-padding tokens
    atten_masks.append(encoded_sentences['attention_mask'])
  # Convert lists to tensors
  ids = torch.cat(ids,dim=0)
  atten_masks = torch.cat(atten_masks,dim=0)  
  labels = torch.tensor(labels)
  return ids,atten_masks,labels 

ids,atten_masks,labels= create_dataset(sentences,labels)



print("Original: ",sentences[1])
print("Token IDs: ",ids[1])

"""All these 0's indicate padding tokens

##### Loading the validation dataset and splitting into tokens
"""

val_data = pd.read_csv('../input/shl123/val_data.csv')
val_ids,val_atten_masks,val_labels = create_dataset(val_data.input.values,val_data.labels.values)

from torch.utils.data import TensorDataset
dataset = TensorDataset(ids,atten_masks,labels)
val_dataset = TensorDataset(val_ids,val_atten_masks,val_labels)

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
# The DataLoader needs to know our batch size for training, so we specify it 
# here. For fine-tuning BERT on a specific task, the authors recommend a batch 
# size of 16 or 32. 
# Reference: https://mccormickml.com/2019/07/22/BERT-fine-tuning/
batch_size = 16

train_dataloader = DataLoader(dataset,sampler=RandomSampler(data),batch_size=batch_size)
val_dataloader = DataLoader(val_dataset,sampler=SequentialSampler(val_data),batch_size=batch_size)

"""### Train Our Classification Model

We use the BertForSequenceClassification model for this task. The relevant details of the model is provided at https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification

We finetune BERT to get our required model
"""

from transformers import BertForSequenceClassification,AdamW,BertConfig 
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', # Using 12 layer BERT model, with an uncased vocab
    num_labels = 2, # Number of o/p labels
    output_attentions=False, # Whether model wil return attn weights
    output_hidden_states=False # whether model returns hidden states
)
# Run the model on GPU
model.cuda()

"""#### Optimizing and Learning rate Schedule

Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.

For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper)

Batch size: 16, 32

Learning rate (Adam): 5e-5, 3e-5, 2e-5

Number of epochs: 2, 3, 4
"""

optimizer = AdamW(model.parameters(),
                  lr=2e-5, # learning rate
                  eps=1e-8 #epsilon: an error tolerance factor
                  )

# Create a learning rate schedule
from transformers import get_linear_schedule_with_warmup 
epochs = 2
total_steps = len(train_dataloader)*epochs
scheduler =get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)

## Helper Functions for F1_score and precision
def f1_score(preds,labels):
  preds = np.argmax(preds,axis=1).flatten()
  labels = labels.flatten()
  tp = np.sum((preds==labels) & (labels==1)) 
  fp = np.sum((preds!=labels) & (preds==1))
  fn = np.sum((preds!=labels) & (preds==0)) 
  precision = tp/(tp+fp)
  recall  = tp/(tp+fn)
  f1 = 2*(precision*recall)/(precision+recall)
  return f1

def precision(preds,labels):
  preds = np.argmax(preds,axis=1).flatten()
  labels = labels.flatten()
  tp = np.sum((labels==1) & (preds==labels)) 
  fp = np.sum((preds!=labels) & (preds==1))
  precision = tp/(tp+fp)
  return precision

device = torch.device("cuda")

training_stats = []
for epoch in range(0,epochs):
  print(f'====== Epoch {epoch} / {epochs}')
  print("Training...")

  train_loss  = 0 
  model.train()
  # Put the model into training mode. Don't be mislead--the call to 
    # `train` just changes the *mode*, it doesn't *perform* the training.
    # `dropout` and `batchnorm` layers behave differently during training
    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-

  for batch in train_dataloader:
    b_ids = batch[0].to(device)
    b_input_mask  = batch[1].to(device)
    b_labels = batch[2].to(device)
    
    # Always clear any previously calculated gradients before performing a
        # backward pass. PyTorch doesn't do this automatically because 
        # accumulating the gradients is "convenient while training RNNs". 
        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-
    model.zero_grad()

    #forward pass
    res = model(b_ids,
                           token_type_ids=None,
                           attention_mask = b_input_mask,
                           labels=b_labels)
    loss,logits  = (res[0],res[1])
    train_loss +=loss.item()


    # Perform a backward pass to calculate the gradients.
    loss.backward()

    # Graient clipping to prevent exploding gradients
    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)

    # Update params
    optimizer.step()

    # Update learning rate
    scheduler.step()

  avg_train_loss = train_loss/len(train_dataloader)
  print(f" Average Training Loss: {avg_train_loss}")

  print("\n Validation....")
  
  model.eval()

  total_eval_precision = 0
  total_eval_f1 = 0
  total_eval_loss = 0
  nb_eval_steps = 0

  for batch in val_dataloader:
     b_input_ids = batch[0].to(device)
     b_input_mask = batch[1].to(device)
     b_labels = batch[2].to(device)
     # Tell pytorch not to bother with constructing the compute graph during
     # the forward pass, since this is only needed for backprop (training).
     with torch.no_grad():
       res = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)
       loss,logits = (res[0],res[1])
     total_eval_loss+=loss.item()
     logits = logits.detach().cpu().numpy()
     label_ids = b_labels.to('cpu').numpy()
     total_eval_precision +=precision(logits,label_ids)
     total_eval_f1 +=f1_score(logits,label_ids)
  avg_val_loss = total_eval_loss/len(val_dataloader)
  avg_precision = total_eval_precision/len(val_dataloader)
  avg_f1 = total_eval_f1/len(val_dataloader)
  print("  Validation Loss: {0:.2f}".format(avg_val_loss))
  print("  Validation Precision: {0:.2f}".format(avg_precision))
  print("  Validation F1: {0:.2f}".format(avg_f1))

  training_stats.append({
      'epoch':epoch+1,
      'Training Loss':avg_train_loss,
      'Valid. Loss': avg_val_loss,
      'Valid. Precision': avg_precision,
      'Valid. F1':avg_f1
  })

stats = pd.DataFrame(training_stats)
stats.set_index('epoch',inplace=True)
stats

import matplotlib.pyplot as plt

import seaborn as sns

# Use plot styling from seaborn.
sns.set(style='darkgrid')
plt.rcParams["figure.figsize"] = (12,6)

# Plot the learning curve.
plt.plot(stats['Training Loss'], 'b-o', label="Training")
plt.plot(stats['Valid. Loss'], 'g-o', label="Validation")
plt.title("Training & Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.xticks([1, 2, 3, 4])

plt.show()

"""### Testing the model"""

import os
output_dir = './model_save/'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
model_to_save = model.module if hasattr(model,'module') else model
model_to_save.save_pretrained(output_dir)

model = BertForSequenceClassification.from_pretrained('./model_save')
model.cuda()

torch.cuda.empty_cache()

test_data = pd.read_csv('../input/shl123/test_data.csv')
sentences = test_data.input.values
output = []
test_ids,test_atten_masks,labels = create_dataset(sentences,[])
test_dataset = TensorDataset(test_ids,test_atten_masks)
test_dataloader = DataLoader(test_dataset,sampler=SequentialSampler(test_dataset),batch_size=batch_size)

model.eval()

# Tracking variables 
predictions = []

# Predict 
for batch in test_dataloader:
  # Add batch to GPU
  batch = tuple(t.to(device) for t in batch)
  
  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask = batch
  
  # Telling the model not to compute or store gradients, saving memory and 
  # speeding up prediction
  with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, token_type_ids=None, 
                      attention_mask=b_input_mask)

  logits = outputs[0]

  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  predictions.append(logits)

out = []
for batch in predictions:
    for pred in batch:
        label = np.argmax(pred)
        out.append(label)

test_data['labels'] =out

test_data.head()

test_data.to_csv('Roushan_Raj_submission.csv')

