{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cbcbf65ad4dd4ae38a342b772fc68a76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9efad84adaa94601b25d90533d8c31dc","IPY_MODEL_7cc2f92fdfae4bd58e6d8fb3e6a9b90d","IPY_MODEL_1b99d3046c52483e84264133300d6e9c"],"layout":"IPY_MODEL_18e668bb5f7c49af826b366b3cf640e3"}},"9efad84adaa94601b25d90533d8c31dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45be85d7189949a0bcc6b33d67645c1d","placeholder":"​","style":"IPY_MODEL_3cedc2d5339d42e3a17973d95ba86717","value":"Downloading: 100%"}},"7cc2f92fdfae4bd58e6d8fb3e6a9b90d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5d769f41cba48c78d34823294443f84","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd9260ae3b694380b3531f2c59da3e3f","value":231508}},"1b99d3046c52483e84264133300d6e9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89c0c81e97e145e0918431ce30fe09cd","placeholder":"​","style":"IPY_MODEL_7109e8ad3a5e4ea2b0e23b97f823a9ac","value":" 232k/232k [00:00&lt;00:00, 1.79MB/s]"}},"18e668bb5f7c49af826b366b3cf640e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45be85d7189949a0bcc6b33d67645c1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cedc2d5339d42e3a17973d95ba86717":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5d769f41cba48c78d34823294443f84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9260ae3b694380b3531f2c59da3e3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89c0c81e97e145e0918431ce30fe09cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7109e8ad3a5e4ea2b0e23b97f823a9ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ff5b79be2c14667a51aad708204a1e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f82f8fe6e4324016a70b3c98d07d1edb","IPY_MODEL_0ca733df92294c07a872366c070bab4f","IPY_MODEL_9b281dab42174d95b341cc25dcf4125a"],"layout":"IPY_MODEL_89eda0b5d19f45fbb8c19f5533d77e29"}},"f82f8fe6e4324016a70b3c98d07d1edb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0016c182dcc4429c993467aa37ab0d47","placeholder":"​","style":"IPY_MODEL_499a5f1f7d214f42bb3bdc9ec7a40a19","value":"Downloading: 100%"}},"0ca733df92294c07a872366c070bab4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b30f1e39e2644149bbad67aecb57deff","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0662efc14c834e929e1fa5f9e23f4796","value":28}},"9b281dab42174d95b341cc25dcf4125a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c66f624148fa4a63b8cce4f989848a52","placeholder":"​","style":"IPY_MODEL_27c69da02e6f4ff1b110ebb55e7f2435","value":" 28.0/28.0 [00:00&lt;00:00, 673B/s]"}},"89eda0b5d19f45fbb8c19f5533d77e29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0016c182dcc4429c993467aa37ab0d47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"499a5f1f7d214f42bb3bdc9ec7a40a19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b30f1e39e2644149bbad67aecb57deff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0662efc14c834e929e1fa5f9e23f4796":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c66f624148fa4a63b8cce4f989848a52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27c69da02e6f4ff1b110ebb55e7f2435":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c6e8b5602234f719065c50e46736280":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_963f87fd579948559a8da69777ea3c4c","IPY_MODEL_a0eae8fcbd714a3d83efd8e21b1e31ca","IPY_MODEL_06ad444ead6f457284ae131876b3b339"],"layout":"IPY_MODEL_76166f4d0f554102bb89816940d0077b"}},"963f87fd579948559a8da69777ea3c4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_563fcf3deefa4496a49a4b579779e7fb","placeholder":"​","style":"IPY_MODEL_ddd613c87b304edf89af5200912c6d24","value":"Downloading: 100%"}},"a0eae8fcbd714a3d83efd8e21b1e31ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_413f3ec95c164ac2b54d88be1ae4f005","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_361a9fda25d347f086841b05c6c793b0","value":570}},"06ad444ead6f457284ae131876b3b339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e605f90ac0564ac192326bfa82204dee","placeholder":"​","style":"IPY_MODEL_e6234c2186c448b18f7d00a5bdc753f1","value":" 570/570 [00:00&lt;00:00, 9.52kB/s]"}},"76166f4d0f554102bb89816940d0077b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"563fcf3deefa4496a49a4b579779e7fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd613c87b304edf89af5200912c6d24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"413f3ec95c164ac2b54d88be1ae4f005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"361a9fda25d347f086841b05c6c793b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e605f90ac0564ac192326bfa82204dee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6234c2186c448b18f7d00a5bdc753f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f44aa782bb6c48a3ac0be1e58a36c8ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03aee16840954174a43c20b6c7da0782","IPY_MODEL_b85a4e539bfa4a6d8f295e74be7879c0","IPY_MODEL_fd55335c3e8f41dba9cf86634fbaf4ee"],"layout":"IPY_MODEL_c7647a49d17a46e2ba196483f8d7ddd1"}},"03aee16840954174a43c20b6c7da0782":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d05e16838d7a4d3caf409a6e986f3459","placeholder":"​","style":"IPY_MODEL_b6f4f56c753e4f88a123730ceddcad57","value":"Downloading: 100%"}},"b85a4e539bfa4a6d8f295e74be7879c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f1b878603b9470aac8dce63ca05f209","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6785a6e800a41d1892cb368a7338afc","value":440473133}},"fd55335c3e8f41dba9cf86634fbaf4ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_460b5f4bc371478998acd12335fe7d48","placeholder":"​","style":"IPY_MODEL_113704788dd045c082a5ccc253c53c26","value":" 440M/440M [00:09&lt;00:00, 45.8MB/s]"}},"c7647a49d17a46e2ba196483f8d7ddd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d05e16838d7a4d3caf409a6e986f3459":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6f4f56c753e4f88a123730ceddcad57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f1b878603b9470aac8dce63ca05f209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6785a6e800a41d1892cb368a7338afc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"460b5f4bc371478998acd12335fe7d48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"113704788dd045c082a5ccc253c53c26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing the libraries","metadata":{"id":"CMK5OdlIxp_B"}},{"cell_type":"code","source":"import torch \nimport numpy as np\nimport pandas as pd","metadata":{"id":"-dF_glisx412","execution":{"iopub.status.busy":"2022-10-16T10:30:17.501754Z","iopub.execute_input":"2022-10-16T10:30:17.502212Z","iopub.status.idle":"2022-10-16T10:30:18.105684Z","shell.execute_reply.started":"2022-10-16T10:30:17.502108Z","shell.execute_reply":"2022-10-16T10:30:18.104395Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtH-oi6Izde6","outputId":"705095d9-0395-48f7-aec5-d0ac42bb7b64","execution":{"iopub.status.busy":"2022-10-16T10:30:18.107238Z","iopub.execute_input":"2022-10-16T10:30:18.107837Z","iopub.status.idle":"2022-10-16T10:30:28.118609Z","shell.execute_reply.started":"2022-10-16T10:30:18.107788Z","shell.execute_reply":"2022-10-16T10:30:28.117405Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### Getting the training and validation data","metadata":{"id":"nS7pwCry8FqZ"}},{"cell_type":"code","source":"# Train data\ndata =  pd.read_csv(\"../input/shl123/train_data.csv\")\ndata.sample(10)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"JagYv90yzh46","outputId":"7dba4828-e607-49f1-e0d0-6aa75f87c98f","execution":{"iopub.status.busy":"2022-10-16T10:30:28.120720Z","iopub.execute_input":"2022-10-16T10:30:28.121125Z","iopub.status.idle":"2022-10-16T10:30:28.166908Z","shell.execute_reply.started":"2022-10-16T10:30:28.121094Z","shell.execute_reply":"2022-10-16T10:30:28.165673Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                   input  labels\n15085              I can not prove our answer is right .       1\n19491                        Contact us ocmda gmail .com       1\n16756                     Many people DON ' T THINK SO .       0\n1559               afterwards will brew along with tea .       1\n8520   Now , I would like to talk about the teacher I...       1\n12630        A natural disaster is not easy to control .       1\n11998                          What do you think of it ?       1\n4179                             Dry it in a sun light .       0\n10164                                            So busy       0\n5585                                           Good news       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15085</th>\n      <td>I can not prove our answer is right .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19491</th>\n      <td>Contact us ocmda gmail .com</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16756</th>\n      <td>Many people DON ' T THINK SO .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1559</th>\n      <td>afterwards will brew along with tea .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8520</th>\n      <td>Now , I would like to talk about the teacher I...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12630</th>\n      <td>A natural disaster is not easy to control .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11998</th>\n      <td>What do you think of it ?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4179</th>\n      <td>Dry it in a sun light .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10164</th>\n      <td>So busy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5585</th>\n      <td>Good news</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Total No. of training sentences\nlen(data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIFfiiPZ7olI","outputId":"3505d4da-3fb3-4a71-ab57-4a69bd31b25e","execution":{"iopub.status.busy":"2022-10-16T10:30:28.170622Z","iopub.execute_input":"2022-10-16T10:30:28.171282Z","iopub.status.idle":"2022-10-16T10:30:28.178096Z","shell.execute_reply.started":"2022-10-16T10:30:28.171242Z","shell.execute_reply":"2022-10-16T10:30:28.176853Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"19998"},"metadata":{}}]},{"cell_type":"code","source":"# List of sentences and their labels\nsentences = data.input.values\nlabels = data.labels.values","metadata":{"id":"Z2i3MRg37xa9","execution":{"iopub.status.busy":"2022-10-16T10:30:28.179874Z","iopub.execute_input":"2022-10-16T10:30:28.180575Z","iopub.status.idle":"2022-10-16T10:30:28.189451Z","shell.execute_reply.started":"2022-10-16T10:30:28.180539Z","shell.execute_reply":"2022-10-16T10:30:28.186771Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"## Check how many negative samples and positive samples there are\nincorrect  = data[data.labels==0]\ncorrect = data[data.labels==1]","metadata":{"id":"o_ORg5E08Anf","execution":{"iopub.status.busy":"2022-10-16T10:30:28.190853Z","iopub.execute_input":"2022-10-16T10:30:28.191766Z","iopub.status.idle":"2022-10-16T10:30:28.200340Z","shell.execute_reply.started":"2022-10-16T10:30:28.191728Z","shell.execute_reply":"2022-10-16T10:30:28.199251Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"0 labelled sentences are: \",len(incorrect))\nprint(\"1 labelled sentences are: \",len(correct))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7CJmiamk8Z_d","outputId":"3febcf4a-be70-4d40-cc0c-3023a8e317da","execution":{"iopub.status.busy":"2022-10-16T10:30:28.201742Z","iopub.execute_input":"2022-10-16T10:30:28.202775Z","iopub.status.idle":"2022-10-16T10:30:28.208365Z","shell.execute_reply.started":"2022-10-16T10:30:28.202739Z","shell.execute_reply":"2022-10-16T10:30:28.207362Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"0 labelled sentences are:  9999\n1 labelled sentences are:  9999\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So we have equal no. of positive and negative samples which ensures no bias of one particular class in the dataset","metadata":{"id":"oNn9qQ4PSZWt"}},{"cell_type":"code","source":"## Helper function to find max length sentence and average length\ndef len_calcs(data):\n  max_len = -1 \n  max_sent = \"\" # Max length sentence\n  lens = 0 # Total words in entire corpus\n  for sentence in sentences:\n    if len(sentence)>max_len:\n      max_len = len(sentence.split(\" \"))\n      max_sent = sentence\n    lens+=len(sentence.split(\" \"))\n  print(f\"The max length sentence is {max_sent} of length {max_len}\")\n  print(f\"The average length is {lens/len(data)}\")\nlen_calcs(data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ix_W8QnM-c7Y","outputId":"4f18612e-fade-4e32-f5c9-b231cdff74ad","execution":{"iopub.status.busy":"2022-10-16T10:30:28.209774Z","iopub.execute_input":"2022-10-16T10:30:28.210782Z","iopub.status.idle":"2022-10-16T10:30:28.245478Z","shell.execute_reply.started":"2022-10-16T10:30:28.210746Z","shell.execute_reply":"2022-10-16T10:30:28.244319Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The max length sentence is Corporate Governance To check whether the meeting records of the Board of Directors and Special Committee were complete and signed by directors who attended the meetings . To check the contents , deadline and signatures of the formal written authorization of shareholder representatives . To check whether scopes of authority were written in letters of authorization by the directors , the letters were signed by directors and the meeting records took down the trustees who attended the meetings . To read all the formal written meeting records and to check whether the frequency of arranging the Board meetings complies with articles of incorporation . To check the meeting records , resolutions and other related contents of Board of Supervisors and to check whether the frequency of arranging the meeting complies with articles of incorporation . To check the meeting records , resolutions and other related contents of the three Special Committees and whether the frequency of arranging the committees complies with the rules of the committees . To check the reports of independent directors of , to check their attendance , giving advice and providing opinions , and status of site work . To read all meeting records of the Board of Directors and check the attendance of in independent directors . To read all the formal written meeting records of the Board of Directors , to check whether the board carried out assessment on operating personnel . To check whether the written notices comply with scheduled time of the articles of incorporation . To check the rules of procedure and reports of three special committees and . To check whether the supervisors are in charge of incompatible positions such as financial staff and senior managers . To check whether the leaders of business units are actually in charge of responsibilities that are supposed to be taken by senior managers , and the status of performing their duties . of length 319\nThe average length is 11.725422542254226\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### BERT TOKENIZER","metadata":{"id":"E-QBunDS8nwk"}},{"cell_type":"markdown","source":"To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\nReference: https://arxiv.org/abs/1810.04805","metadata":{"id":"MsPihIE4TBpk"}},{"cell_type":"code","source":"# Loading the BERT Tokenizer\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_cased=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["cbcbf65ad4dd4ae38a342b772fc68a76","9efad84adaa94601b25d90533d8c31dc","7cc2f92fdfae4bd58e6d8fb3e6a9b90d","1b99d3046c52483e84264133300d6e9c","18e668bb5f7c49af826b366b3cf640e3","45be85d7189949a0bcc6b33d67645c1d","3cedc2d5339d42e3a17973d95ba86717","d5d769f41cba48c78d34823294443f84","cd9260ae3b694380b3531f2c59da3e3f","89c0c81e97e145e0918431ce30fe09cd","7109e8ad3a5e4ea2b0e23b97f823a9ac","9ff5b79be2c14667a51aad708204a1e1","f82f8fe6e4324016a70b3c98d07d1edb","0ca733df92294c07a872366c070bab4f","9b281dab42174d95b341cc25dcf4125a","89eda0b5d19f45fbb8c19f5533d77e29","0016c182dcc4429c993467aa37ab0d47","499a5f1f7d214f42bb3bdc9ec7a40a19","b30f1e39e2644149bbad67aecb57deff","0662efc14c834e929e1fa5f9e23f4796","c66f624148fa4a63b8cce4f989848a52","27c69da02e6f4ff1b110ebb55e7f2435","5c6e8b5602234f719065c50e46736280","963f87fd579948559a8da69777ea3c4c","a0eae8fcbd714a3d83efd8e21b1e31ca","06ad444ead6f457284ae131876b3b339","76166f4d0f554102bb89816940d0077b","563fcf3deefa4496a49a4b579779e7fb","ddd613c87b304edf89af5200912c6d24","413f3ec95c164ac2b54d88be1ae4f005","361a9fda25d347f086841b05c6c793b0","e605f90ac0564ac192326bfa82204dee","e6234c2186c448b18f7d00a5bdc753f1"]},"id":"_4tQovvf83k3","outputId":"2a630d4c-8b75-4ae8-f8ae-88d3488e7b0c","execution":{"iopub.status.busy":"2022-10-16T10:30:28.247307Z","iopub.execute_input":"2022-10-16T10:30:28.247755Z","iopub.status.idle":"2022-10-16T10:30:28.903314Z","shell.execute_reply.started":"2022-10-16T10:30:28.247719Z","shell.execute_reply":"2022-10-16T10:30:28.902304Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Original sentence\nprint('Original: ',sentences[1])\n\n# Sentence split into tokens\n\nprint(\"Tokenized: \",tokenizer.tokenize(sentences[1]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLNGzPAR9ENS","outputId":"fd885f59-9a37-4d61-99d9-03741cf7e03e","execution":{"iopub.status.busy":"2022-10-16T10:30:28.906371Z","iopub.execute_input":"2022-10-16T10:30:28.907090Z","iopub.status.idle":"2022-10-16T10:30:28.914771Z","shell.execute_reply.started":"2022-10-16T10:30:28.907050Z","shell.execute_reply":"2022-10-16T10:30:28.913553Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Original:  I am not interested in cars or electric appliances .\nTokenized:  ['i', 'am', 'not', 'interested', 'in', 'cars', 'or', 'electric', 'appliances', '.']\n","output_type":"stream"}]},{"cell_type":"code","source":"## We now tokenize the entire training dataset and map the tokens to their ID's\n\ndef create_dataset(sentences,labels):\n  ids = []\n  atten_masks = []\n\n  for sentence in sentences:\n    ''' encode_plus: (1) Tokenize the sentence\n                    (2) Prepend [CLS] start token to the beginning to indicate start of sentence\n                    (3) Append [SEP] end token to the end\n                    (4) Map tokens to IDs\n                    (5) Pad or truncate sentence to max_length\n                    (6) Create attention masks to ignore [PAD] tokens\n    '''\n    encoded_sentences = tokenizer.encode_plus(sentence,\n                                              add_special_tokens=True, #Add [CLS and [SEP]\n                                              max_length=128,\n                                              pad_to_max_length=True,\n                                              return_attention_mask = True,\n                                              return_tensors = 'pt' # Return pytorch tensors\n                                              )\n    # Add the encoded sentence to the list. \n    ids.append(encoded_sentences['input_ids'])\n    # Attention mask differentiates padding from non-padding tokens\n    atten_masks.append(encoded_sentences['attention_mask'])\n  # Convert lists to tensors\n  ids = torch.cat(ids,dim=0)\n  atten_masks = torch.cat(atten_masks,dim=0)  \n  labels = torch.tensor(labels)\n  return ids,atten_masks,labels \n\nids,atten_masks,labels= create_dataset(sentences,labels)\n\n\n\nprint(\"Original: \",sentences[1])\nprint(\"Token IDs: \",ids[1])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eKX1Iy7H9TnU","outputId":"f327704d-61da-4f6d-acb8-0eeabef54a72","execution":{"iopub.status.busy":"2022-10-16T10:30:28.916772Z","iopub.execute_input":"2022-10-16T10:30:28.917178Z","iopub.status.idle":"2022-10-16T10:30:39.898586Z","shell.execute_reply.started":"2022-10-16T10:30:28.917139Z","shell.execute_reply":"2022-10-16T10:30:39.897494Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Original:  I am not interested in cars or electric appliances .\nToken IDs:  tensor([  101,  1045,  2572,  2025,  4699,  1999,  3765,  2030,  3751, 22449,\n         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"All these 0's indicate padding tokens","metadata":{"id":"WstXDy0KACFB"}},{"cell_type":"markdown","source":"##### Loading the validation dataset and splitting into tokens","metadata":{"id":"fUjfdrtqTgzK"}},{"cell_type":"code","source":"val_data = pd.read_csv('../input/shl123/val_data.csv')\nval_ids,val_atten_masks,val_labels = create_dataset(val_data.input.values,val_data.labels.values)","metadata":{"id":"Zbs4_90EAoTR","execution":{"iopub.status.busy":"2022-10-16T10:30:39.903612Z","iopub.execute_input":"2022-10-16T10:30:39.906125Z","iopub.status.idle":"2022-10-16T10:30:45.860227Z","shell.execute_reply.started":"2022-10-16T10:30:39.906083Z","shell.execute_reply":"2022-10-16T10:30:45.858942Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\ndataset = TensorDataset(ids,atten_masks,labels)\nval_dataset = TensorDataset(val_ids,val_atten_masks,val_labels)","metadata":{"id":"BV1nd5ARK-06","execution":{"iopub.status.busy":"2022-10-16T10:30:45.865835Z","iopub.execute_input":"2022-10-16T10:30:45.866185Z","iopub.status.idle":"2022-10-16T10:30:45.872299Z","shell.execute_reply.started":"2022-10-16T10:30:45.866152Z","shell.execute_reply":"2022-10-16T10:30:45.870971Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n# The DataLoader needs to know our batch size for training, so we specify it \n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n# size of 16 or 32. \n# Reference: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\nbatch_size = 16\n\ntrain_dataloader = DataLoader(dataset,sampler=RandomSampler(data),batch_size=batch_size)\nval_dataloader = DataLoader(val_dataset,sampler=SequentialSampler(val_data),batch_size=batch_size)\n\n","metadata":{"id":"Qj8r-mo3_yW1","execution":{"iopub.status.busy":"2022-10-16T10:30:45.874641Z","iopub.execute_input":"2022-10-16T10:30:45.875056Z","iopub.status.idle":"2022-10-16T10:30:45.887478Z","shell.execute_reply.started":"2022-10-16T10:30:45.875016Z","shell.execute_reply":"2022-10-16T10:30:45.886398Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Train Our Classification Model","metadata":{"id":"9FFaM3hRA4Vy"}},{"cell_type":"markdown","source":"We use the BertForSequenceClassification model for this task. The relevant details of the model is provided at https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification\n\nWe finetune BERT to get our required model","metadata":{"id":"rF6qa8YZUP6b"}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification,AdamW,BertConfig \nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased', # Using 12 layer BERT model, with an uncased vocab\n    num_labels = 2, # Number of o/p labels\n    output_attentions=False, # Whether model wil return attn weights\n    output_hidden_states=False # whether model returns hidden states\n)\n# Run the model on GPU\nmodel.cuda()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484,"referenced_widgets":["f44aa782bb6c48a3ac0be1e58a36c8ba","03aee16840954174a43c20b6c7da0782","b85a4e539bfa4a6d8f295e74be7879c0","fd55335c3e8f41dba9cf86634fbaf4ee","c7647a49d17a46e2ba196483f8d7ddd1","d05e16838d7a4d3caf409a6e986f3459","b6f4f56c753e4f88a123730ceddcad57","1f1b878603b9470aac8dce63ca05f209","a6785a6e800a41d1892cb368a7338afc","460b5f4bc371478998acd12335fe7d48","113704788dd045c082a5ccc253c53c26"]},"id":"Bh6FmVsVA8sd","outputId":"e2d2564b-2857-4d06-82d1-7c5e59ef7564","execution":{"iopub.status.busy":"2022-10-16T10:30:45.889254Z","iopub.execute_input":"2022-10-16T10:30:45.889830Z","iopub.status.idle":"2022-10-16T10:30:52.397534Z","shell.execute_reply.started":"2022-10-16T10:30:45.889643Z","shell.execute_reply":"2022-10-16T10:30:52.396317Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Optimizing and Learning rate Schedule","metadata":{"id":"gECzG6A9Bc9b"}},{"cell_type":"markdown","source":"Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n\nFor the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper)\n\nBatch size: 16, 32\n\nLearning rate (Adam): 5e-5, 3e-5, 2e-5\n\nNumber of epochs: 2, 3, 4","metadata":{"id":"71aLVxsaBvw4"}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr=2e-5, # learning rate\n                  eps=1e-8 #epsilon: an error tolerance factor\n                  )","metadata":{"id":"Lxu12httBtty","execution":{"iopub.status.busy":"2022-10-16T10:30:52.398986Z","iopub.execute_input":"2022-10-16T10:30:52.399417Z","iopub.status.idle":"2022-10-16T10:30:52.410030Z","shell.execute_reply.started":"2022-10-16T10:30:52.399374Z","shell.execute_reply":"2022-10-16T10:30:52.408872Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a learning rate schedule\nfrom transformers import get_linear_schedule_with_warmup \nepochs = 2\ntotal_steps = len(train_dataloader)*epochs\nscheduler =get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)","metadata":{"id":"bFgiuXi8CBCk","execution":{"iopub.status.busy":"2022-10-16T10:30:52.412086Z","iopub.execute_input":"2022-10-16T10:30:52.412790Z","iopub.status.idle":"2022-10-16T10:30:52.419203Z","shell.execute_reply.started":"2022-10-16T10:30:52.412753Z","shell.execute_reply":"2022-10-16T10:30:52.417914Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## Helper Functions for F1_score and precision\ndef f1_score(preds,labels):\n  preds = np.argmax(preds,axis=1).flatten()\n  labels = labels.flatten()\n  tp = np.sum((preds==labels) & (labels==1)) \n  fp = np.sum((preds!=labels) & (preds==1))\n  fn = np.sum((preds!=labels) & (preds==0)) \n  precision = tp/(tp+fp)\n  recall  = tp/(tp+fn)\n  f1 = 2*(precision*recall)/(precision+recall)\n  return f1\n\ndef precision(preds,labels):\n  preds = np.argmax(preds,axis=1).flatten()\n  labels = labels.flatten()\n  tp = np.sum((labels==1) & (preds==labels)) \n  fp = np.sum((preds!=labels) & (preds==1))\n  precision = tp/(tp+fp)\n  return precision","metadata":{"id":"GAkenWfBCasA","execution":{"iopub.status.busy":"2022-10-16T10:30:52.420793Z","iopub.execute_input":"2022-10-16T10:30:52.422060Z","iopub.status.idle":"2022-10-16T10:30:52.452781Z","shell.execute_reply.started":"2022-10-16T10:30:52.422014Z","shell.execute_reply":"2022-10-16T10:30:52.451201Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"id":"CM3HUM3NLY6C","execution":{"iopub.status.busy":"2022-10-16T10:30:52.455656Z","iopub.execute_input":"2022-10-16T10:30:52.456090Z","iopub.status.idle":"2022-10-16T10:30:52.462828Z","shell.execute_reply.started":"2022-10-16T10:30:52.456027Z","shell.execute_reply":"2022-10-16T10:30:52.461606Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"training_stats = []\nfor epoch in range(0,epochs):\n  print(f'====== Epoch {epoch} / {epochs}')\n  print(\"Training...\")\n\n  train_loss  = 0 \n  model.train()\n  # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-\n\n  for batch in train_dataloader:\n    b_ids = batch[0].to(device)\n    b_input_mask  = batch[1].to(device)\n    b_labels = batch[2].to(device)\n    \n    # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-\n    model.zero_grad()\n\n    #forward pass\n    res = model(b_ids,\n                           token_type_ids=None,\n                           attention_mask = b_input_mask,\n                           labels=b_labels)\n    loss,logits  = (res[0],res[1])\n    train_loss +=loss.item()\n\n\n    # Perform a backward pass to calculate the gradients.\n    loss.backward()\n\n    # Graient clipping to prevent exploding gradients\n    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n\n    # Update params\n    optimizer.step()\n\n    # Update learning rate\n    scheduler.step()\n\n  avg_train_loss = train_loss/len(train_dataloader)\n  print(f\" Average Training Loss: {avg_train_loss}\")\n\n  print(\"\\n Validation....\")\n  \n  model.eval()\n\n  total_eval_precision = 0\n  total_eval_f1 = 0\n  total_eval_loss = 0\n  nb_eval_steps = 0\n\n  for batch in val_dataloader:\n     b_input_ids = batch[0].to(device)\n     b_input_mask = batch[1].to(device)\n     b_labels = batch[2].to(device)\n     # Tell pytorch not to bother with constructing the compute graph during\n     # the forward pass, since this is only needed for backprop (training).\n     with torch.no_grad():\n       res = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)\n       loss,logits = (res[0],res[1])\n     total_eval_loss+=loss.item()\n     logits = logits.detach().cpu().numpy()\n     label_ids = b_labels.to('cpu').numpy()\n     total_eval_precision +=precision(logits,label_ids)\n     total_eval_f1 +=f1_score(logits,label_ids)\n  avg_val_loss = total_eval_loss/len(val_dataloader)\n  avg_precision = total_eval_precision/len(val_dataloader)\n  avg_f1 = total_eval_f1/len(val_dataloader)\n  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n  print(\"  Validation Precision: {0:.2f}\".format(avg_precision))\n  print(\"  Validation F1: {0:.2f}\".format(avg_f1))\n\n  training_stats.append({\n      'epoch':epoch+1,\n      'Training Loss':avg_train_loss,\n      'Valid. Loss': avg_val_loss,\n      'Valid. Precision': avg_precision,\n      'Valid. F1':avg_f1\n  })\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yz-4dr8GDlc4","outputId":"25f03d13-0642-4812-8a26-d519fb5bdb16","execution":{"iopub.status.busy":"2022-10-16T10:30:52.464784Z","iopub.execute_input":"2022-10-16T10:30:52.465374Z","iopub.status.idle":"2022-10-16T10:40:37.015800Z","shell.execute_reply.started":"2022-10-16T10:30:52.465313Z","shell.execute_reply":"2022-10-16T10:40:37.014634Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"====== Epoch 0 / 2\nTraining...\n Average Training Loss: 0.6594457279443741\n\n Validation....\n  Validation Loss: 0.66\n  Validation Precision: 0.56\n  Validation F1: 0.68\n====== Epoch 1 / 2\nTraining...\n Average Training Loss: 0.5883319868564606\n\n Validation....\n  Validation Loss: 0.69\n  Validation Precision: 0.56\n  Validation F1: 0.66\n","output_type":"stream"}]},{"cell_type":"code","source":"stats = pd.DataFrame(training_stats)\nstats.set_index('epoch',inplace=True)\nstats","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"wFqqNWXvFRbm","outputId":"6d186ea9-7254-43ae-8b47-f460e11034ea","execution":{"iopub.status.busy":"2022-10-16T10:41:53.515324Z","iopub.execute_input":"2022-10-16T10:41:53.516281Z","iopub.status.idle":"2022-10-16T10:41:53.533812Z","shell.execute_reply.started":"2022-10-16T10:41:53.516193Z","shell.execute_reply":"2022-10-16T10:41:53.532395Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"       Training Loss  Valid. Loss  Valid. Precision  Valid. F1\nepoch                                                         \n1           0.659446     0.664797          0.557294   0.683051\n2           0.588332     0.692627          0.562185   0.660756","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Valid. Precision</th>\n      <th>Valid. F1</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.659446</td>\n      <td>0.664797</td>\n      <td>0.557294</td>\n      <td>0.683051</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.588332</td>\n      <td>0.692627</td>\n      <td>0.562185</td>\n      <td>0.660756</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(stats['Valid. Loss'], 'g-o', label=\"Validation\")\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"id":"SfYpwpg-WIKo","outputId":"de2d05ff-6f2c-43aa-c9c5-5903624c0e61","execution":{"iopub.status.busy":"2022-10-16T10:41:54.849014Z","iopub.execute_input":"2022-10-16T10:41:54.849461Z","iopub.status.idle":"2022-10-16T10:41:55.325480Z","shell.execute_reply.started":"2022-10-16T10:41:54.849420Z","shell.execute_reply":"2022-10-16T10:41:55.324330Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### Testing the model","metadata":{"id":"mK-ORPqoWPzZ"}},{"cell_type":"code","source":"import os\noutput_dir = './model_save/'\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nmodel_to_save = model.module if hasattr(model,'module') else model\nmodel_to_save.save_pretrained(output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-16T10:41:57.238094Z","iopub.execute_input":"2022-10-16T10:41:57.238479Z","iopub.status.idle":"2022-10-16T10:41:58.326100Z","shell.execute_reply.started":"2022-10-16T10:41:57.238445Z","shell.execute_reply":"2022-10-16T10:41:58.324443Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('./model_save')\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-10-16T10:41:59.394721Z","iopub.execute_input":"2022-10-16T10:41:59.395539Z","iopub.status.idle":"2022-10-16T10:42:01.030224Z","shell.execute_reply.started":"2022-10-16T10:41:59.395498Z","shell.execute_reply":"2022-10-16T10:42:01.029090Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-10-16T10:42:01.032362Z","iopub.execute_input":"2022-10-16T10:42:01.032841Z","iopub.status.idle":"2022-10-16T10:42:01.086647Z","shell.execute_reply.started":"2022-10-16T10:42:01.032802Z","shell.execute_reply":"2022-10-16T10:42:01.084943Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/shl123/test_data.csv')\nsentences = test_data.input.values\noutput = []\ntest_ids,test_atten_masks,labels = create_dataset(sentences,[])\ntest_dataset = TensorDataset(test_ids,test_atten_masks)\ntest_dataloader = DataLoader(test_dataset,sampler=SequentialSampler(test_dataset),batch_size=batch_size)","metadata":{"id":"k9e4bv9nW471","execution":{"iopub.status.busy":"2022-10-16T10:42:01.088566Z","iopub.execute_input":"2022-10-16T10:42:01.088987Z","iopub.status.idle":"2022-10-16T10:42:06.758063Z","shell.execute_reply.started":"2022-10-16T10:42:01.088946Z","shell.execute_reply":"2022-10-16T10:42:06.756935Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\n\n# Tracking variables \npredictions = []\n\n# Predict \nfor batch in test_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n  logits = outputs[0]\n\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  predictions.append(logits)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFgNXjwEXVMc","outputId":"dc53692f-5e85-407a-ad1e-9450e5d9c7af","execution":{"iopub.status.busy":"2022-10-16T10:42:06.760548Z","iopub.execute_input":"2022-10-16T10:42:06.761257Z","iopub.status.idle":"2022-10-16T10:42:43.844174Z","shell.execute_reply.started":"2022-10-16T10:42:06.761213Z","shell.execute_reply":"2022-10-16T10:42:43.843135Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{"id":"0HWZbjcvXrDp"}},{"cell_type":"code","source":"out = []\nfor batch in predictions:\n    for pred in batch:\n        label = np.argmax(pred)\n        out.append(label)","metadata":{"id":"bNd94oCJZHbY","execution":{"iopub.status.busy":"2022-10-16T10:42:43.845781Z","iopub.execute_input":"2022-10-16T10:42:43.846195Z","iopub.status.idle":"2022-10-16T10:42:43.879687Z","shell.execute_reply.started":"2022-10-16T10:42:43.846155Z","shell.execute_reply":"2022-10-16T10:42:43.878595Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"test_data['labels'] =out","metadata":{"execution":{"iopub.status.busy":"2022-10-16T10:42:43.881587Z","iopub.execute_input":"2022-10-16T10:42:43.882020Z","iopub.status.idle":"2022-10-16T10:42:43.891440Z","shell.execute_reply.started":"2022-10-16T10:42:43.881980Z","shell.execute_reply":"2022-10-16T10:42:43.890322Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-16T10:42:43.894226Z","iopub.execute_input":"2022-10-16T10:42:43.894915Z","iopub.status.idle":"2022-10-16T10:42:43.907311Z","shell.execute_reply.started":"2022-10-16T10:42:43.894874Z","shell.execute_reply":"2022-10-16T10:42:43.906138Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                               input  labels\n0  I believe they will master Japanese soon becau...       1\n1                              I am looking for it .       1\n2  Apple is a round fruit with smooth and colorfu...       0\n3                              Let It Will Be Push .       0\n4                  I rode on this ship from Sendai .       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I believe they will master Japanese soon becau...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I am looking for it .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Apple is a round fruit with smooth and colorfu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Let It Will Be Push .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I rode on this ship from Sendai .</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.to_csv('Roushan_Raj_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-10-16T10:43:55.868293Z","iopub.execute_input":"2022-10-16T10:43:55.868973Z","iopub.status.idle":"2022-10-16T10:43:55.921795Z","shell.execute_reply.started":"2022-10-16T10:43:55.868924Z","shell.execute_reply":"2022-10-16T10:43:55.920638Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}